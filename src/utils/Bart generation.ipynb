{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 364,
     "status": "ok",
     "timestamp": 1641832764831,
     "user": {
      "displayName": "Diya A",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03482944229402493818"
     },
     "user_tz": -540
    },
    "id": "5-Ds4f_PK9Jg",
    "outputId": "e4e1e733-233f-4477-b1dc-ffa2497264f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/OpenNMT-kpg-release-master/models\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/MyDrive/OpenNMT-kpg-release-master/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13520,
     "status": "ok",
     "timestamp": 1641832779387,
     "user": {
      "displayName": "Diya A",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03482944229402493818"
     },
     "user_tz": -540
    },
    "id": "fh3Dtc3HLhfX",
    "outputId": "140b50bc-3983-46e1-ddf7-99d352725c5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-1.17.0-py3-none-any.whl (306 kB)\n",
      "\u001b[K     |████████████████████████████████| 306 kB 5.2 MB/s \n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 47.3 MB/s \n",
      "\u001b[?25hCollecting rouge-score\n",
      "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
      "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 471 kB/s \n",
      "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 51.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.2)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 41.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 52.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Collecting pyyaml\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[K     |████████████████████████████████| 596 kB 46.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 44.3 MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 47.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (0.12.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
      "Collecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
      "\u001b[K     |████████████████████████████████| 192 kB 47.2 MB/s \n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
      "\u001b[K     |████████████████████████████████| 160 kB 51.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.9)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
      "\u001b[K     |████████████████████████████████| 271 kB 46.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, pyyaml, fsspec, aiohttp, xxhash, tokenizers, sacremoses, huggingface-hub, transformers, rouge-score, datasets\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-1.17.0 frozenlist-1.2.0 fsspec-2021.11.1 huggingface-hub-0.2.1 multidict-5.2.0 pyyaml-6.0 rouge-score-0.0.4 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.15.0 xxhash-2.0.2 yarl-1.7.2\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets transformers rouge-score nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1641832779388,
     "user": {
      "displayName": "Diya A",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03482944229402493818"
     },
     "user_tz": -540
    },
    "id": "sl7JBBgtbGCt",
    "outputId": "10aec641-5498-4d41-c256-acaa2e24c32f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan 10 16:39:37 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   66C    P8    32W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 973,
     "status": "ok",
     "timestamp": 1637779047452,
     "user": {
      "displayName": "Diya A",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03482944229402493818"
     },
     "user_tz": -540
    },
    "id": "1MOah0PVi_Si",
    "outputId": "151f69e7-f264-4769-e4f8-563f6216b4b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1271,
     "status": "ok",
     "timestamp": 1637779050027,
     "user": {
      "displayName": "Diya A",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03482944229402493818"
     },
     "user_tz": -540
    },
    "id": "SwFD0esHiXtd",
    "outputId": "5c9bea84-85a9-4563-a731-0cf28455387d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precise', 'accurate', 'take', 'exact', 'demand', 'claim'}\n",
      "{'penalty', 'penalisation', 'penalization', 'punishment'}\n",
      "{'mathematical_function', 'work', 'use', 'occasion', 'social_function', 'function', 'subroutine', 'social_occasion', 'go', 'procedure', 'single-valued_function', 'map', 'operate', 'affair', 'part', 'mapping', 'purpose', 'role', 'run', 'routine', 'serve', 'officiate', 'office', 'subprogram'}\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "text = \"exact penalty functions\"\n",
    "splited_text = (text.split(\" \"))\n",
    "for splited_item in splited_text:\n",
    "  synonyms = wordnet.synsets(splited_item)\n",
    "  print (set(chain.from_iterable([word.lemma_names() for word in synonyms])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 38564,
     "status": "ok",
     "timestamp": 1641832824780,
     "user": {
      "displayName": "Diya A",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03482944229402493818"
     },
     "user_tz": -540
    },
    "id": "R6ocgd8CLpst"
   },
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
    "import json\n",
    "import torch\n",
    "import ast\n",
    "\n",
    "device = 'cuda'#'cpu\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained('KPBL-1028').to(device)#BART-large-Finetuned\n",
    "tokenizer = BartTokenizer.from_pretrained('KPBL-1028')\n",
    "#tokenizer = BartTokenizer.from_pretrained('added')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1641793753899,
     "user": {
      "displayName": "Diya A",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03482944229402493818"
     },
     "user_tz": -540
    },
    "id": "GXRxUUB5kgFY"
   },
   "outputs": [],
   "source": [
    "ss = tokenizer(\"Keyphrases are condensed summaries of text information, representing a document's theme and core idea. Keyphrase extraction approaches based on statistics and deep learning have received widespread attention. However, the extractive approaches have obvious limitations like keyphrases must occur in the document. However, a considerable amount of human-annotated keyphrases are absent keyphrases such that they are not occurring in the source document explicitly, which cannot be extracted by the extractive methods. Keyphrase generation is based on a sequence-to-sequence deep learning model, generating present and absent keyphrases from the semantic representation of the document. However, recall and precision of generated abstract keyphrases still need to be improved. We propose a two-step keyphrase generation model to solve these problems, which consists of a finetuned BART-based keyphrase generator, coupled with a BERT cross encoder which ranks generated keyphrases. Our model can generate absent keyphrases and shows superior results over the strong baselines in four widely used datasets.\")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1641793755948,
     "user": {
      "displayName": "Diya A",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03482944229402493818"
     },
     "user_tz": -540
    },
    "id": "20x2pnCjEftH",
    "outputId": "d2922624-0226-4195-df1c-cb3667dafefa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219\n"
     ]
    }
   ],
   "source": [
    "print (len(ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1270,
     "status": "ok",
     "timestamp": 1637678378136,
     "user": {
      "displayName": "Diya A",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03482944229402493818"
     },
     "user_tz": -540
    },
    "id": "X1Hy44ZVyTcA",
    "outputId": "df3c62e9-2d18-46ef-d14a-9d7c3a52b5e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 35760,   642,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1]], device='cuda:0')}\n",
      "['[\"information storage and retrieval\", \"natural language processing\", \"artificial intelligence\", \"computer vision\", \"pattern recognition\", \"machine learning\", \"neural networks\", \"support vector machines\", \"genetic algorithms\", \"probabilistic models\", \"statistical models\", \"<digit> <digit>\", \"data mining\"]']\n"
     ]
    }
   ],
   "source": [
    "tinputs = tokenizer([\"nlp\"], return_tensors='pt', truncation=True).to(device)#, padding=True\n",
    "print (tinputs)\n",
    "\n",
    "# Generate Summary\n",
    "tsummary_ids = model.generate(tinputs['input_ids'],max_length = 256,min_length = 64,num_beams = 7).to(device)  #length_penalty = 3.0  top_k = 5\n",
    "tbart_pred = str([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in tsummary_ids])#[2:-2]\n",
    "print (tbart_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1637664097361,
     "user": {
      "displayName": "Diya A",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03482944229402493818"
     },
     "user_tz": -540
    },
    "id": "7xU8FQhWYtwi",
    "outputId": "b411b88e-d15a-468a-e228-c6fdc70500fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('added/tokenizer_config.json',\n",
       " 'added/special_tokens_map.json',\n",
       " 'added/vocab.json',\n",
       " 'added/merges.txt',\n",
       " 'added/added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 428,
     "status": "ok",
     "timestamp": 1637664061295,
     "user": {
      "displayName": "Diya A",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03482944229402493818"
     },
     "user_tz": -540
    },
    "id": "7PcZ3gkCUIxa",
    "outputId": "be6f29d0-802c-4a9a-b1af-80f1a1d60078"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vocabulary path (added) should be a directory\n"
     ]
    }
   ],
   "source": [
    "tokenizer.save_vocabulary(\"added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 208,
     "status": "ok",
     "timestamp": 1639325739774,
     "user": {
      "displayName": "Diya A",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03482944229402493818"
     },
     "user_tz": -540
    },
    "id": "k2x2QE-ucV3z",
    "outputId": "6e5c6be9-3051-462b-cf1b-8c30601fdf32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50265"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1639325741060,
     "user": {
      "displayName": "Diya A",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03482944229402493818"
     },
     "user_tz": -540
    },
    "id": "P2Gs3ETAc74c",
    "outputId": "f87f07e6-67c1-4e41-db7e-761094bbdad1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 225,
     "status": "ok",
     "timestamp": 1639325747660,
     "user": {
      "displayName": "Diya A",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03482944229402493818"
     },
     "user_tz": -540
    },
    "id": "_vBzPKwTmprH",
    "outputId": "d7ff0d6f-6dbf-4e6e-a0d9-b17b7d535971"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_added_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1641793788374,
     "user": {
      "displayName": "Diya A",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03482944229402493818"
     },
     "user_tz": -540
    },
    "id": "sQ_1d85EgTi8"
   },
   "outputs": [],
   "source": [
    "def bart_pred(src):\n",
    "  ARTICLE_TO_SUMMARIZE = src\n",
    "  inputs = tokenizer([ARTICLE_TO_SUMMARIZE], return_tensors='pt', truncation=True).to(device)#, padding=True\n",
    "\n",
    "\n",
    "  # Generate Summary\n",
    "  summary_ids = model.generate(inputs['input_ids'],max_length = 256,min_length = 64,num_beams = 7).to(device)  #length_penalty = 3.0  top_k = 5\n",
    "  bart_pred = str([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in summary_ids])#[2:-2]\n",
    "  return (bart_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 578,
     "status": "ok",
     "timestamp": 1639325828245,
     "user": {
      "displayName": "Diya A",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03482944229402493818"
     },
     "user_tz": -540
    },
    "id": "O-pBNeJspUKd",
    "outputId": "58733b92-c127-48f7-b70c-838045eb1334"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/OpenNMT-kpg-release-master/data/keyphrase/meng17\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/MyDrive/OpenNMT-kpg-release-master/data/keyphrase/meng17/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzAf1Yf_hVfw"
   },
   "outputs": [],
   "source": [
    "#Original!\n",
    "def get_pred_BART(dataset_name):\n",
    "    data = []\n",
    "    with open(dataset_name+ '/' +dataset_name+ \"_test.src\", 'r+') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "        global data_len\n",
    "        data_len = len(data)\n",
    "        for i in range(len(data)):\n",
    "          dic = data[i]\n",
    "          if i % 50 == 0:          \n",
    "            print(dataset_name + \" ：\" + str(i) + \" / \" + str(len(data)))\n",
    "\n",
    "          src_value = dic[\"src\"]\n",
    "          bart_tgt = bart_pred(src_value)\n",
    "          dic[\"pred\"] = bart_tgt.replace ('\\\\','')\n",
    "          del dic['title']\n",
    "          del dic['src']\n",
    "          del dic['abstract']\n",
    "          # print(dic)\n",
    "          with open(dataset_name+ '/' +dataset_name+ \"_BL1028.pred\",'a+') as t:\n",
    "              json.dump(dic,t)\n",
    "              t.write('\\n')\n",
    "        f.close()\n",
    "        t.close()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6Gf0Hg_xx3s"
   },
   "outputs": [],
   "source": [
    "#domain corpus xxx\n",
    "def get_pred_BART_domain(dataset_name):\n",
    "    data = []\n",
    "    generated = []\n",
    "    with open(dataset_name+ '/' +dataset_name+ \"_test.src\", 'r+') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "        global data_len\n",
    "        data_len = len(data)\n",
    "        for i in range(len(data)):\n",
    "          dic = data[i]\n",
    "          if i % 50 == 0:          \n",
    "            print(dataset_name + \" ：\" + str(i) + \" / \" + str(len(data)))\n",
    "            \n",
    "\n",
    "          src_value = dic[\"src\"]# + str(generated)\n",
    "          bart_tgt = bart_pred(src_value)[2:-2]\n",
    "          \n",
    "\n",
    "          try:\n",
    "              generated.append(ast.literal_eval(bart_tgt))\n",
    "          except:\n",
    "              print(\"list error\")\n",
    "          \n",
    "          \n",
    "          # generated += domain_tgt[0:2]\n",
    "          # generated = set(generated)\n",
    "          # generated = list(generated)\n",
    "\n",
    "          dic[\"pred\"] = bart_tgt.replace ('\\\\','')\n",
    "          del dic['title']\n",
    "          del dic['src']\n",
    "          del dic['abstract']\n",
    "          \n",
    "          with open(dataset_name+ '/' +dataset_name+ \"_tokenizerBL1028.pred\",'a+') as t:\n",
    "              json.dump(dic,t)\n",
    "              t.write('\\n')\n",
    "        for item in generated:\n",
    "            print(item)\n",
    "            tokenizer.add_tokens(item)\n",
    "        f.close()\n",
    "        t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2287323,
     "status": "ok",
     "timestamp": 1639333404109,
     "user": {
      "displayName": "Diya A",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03482944229402493818"
     },
     "user_tz": -540
    },
    "id": "mwplgL-Q9SGq",
    "outputId": "29b0b71c-069b-4d00-b2f4-39d76541ccb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kp20k_valid2k ：0 / 2000\n",
      "kp20k_valid2k ：50 / 2000\n",
      "kp20k_valid2k ：100 / 2000\n",
      "kp20k_valid2k ：150 / 2000\n",
      "kp20k_valid2k ：200 / 2000\n",
      "kp20k_valid2k ：250 / 2000\n",
      "kp20k_valid2k ：300 / 2000\n",
      "kp20k_valid2k ：350 / 2000\n",
      "kp20k_valid2k ：400 / 2000\n",
      "kp20k_valid2k ：450 / 2000\n",
      "kp20k_valid2k ：500 / 2000\n",
      "kp20k_valid2k ：550 / 2000\n",
      "kp20k_valid2k ：600 / 2000\n",
      "kp20k_valid2k ：650 / 2000\n",
      "kp20k_valid2k ：700 / 2000\n",
      "kp20k_valid2k ：750 / 2000\n",
      "kp20k_valid2k ：800 / 2000\n",
      "kp20k_valid2k ：850 / 2000\n",
      "kp20k_valid2k ：900 / 2000\n",
      "kp20k_valid2k ：950 / 2000\n",
      "kp20k_valid2k ：1000 / 2000\n",
      "kp20k_valid2k ：1050 / 2000\n",
      "kp20k_valid2k ：1100 / 2000\n",
      "kp20k_valid2k ：1150 / 2000\n",
      "kp20k_valid2k ：1200 / 2000\n",
      "kp20k_valid2k ：1250 / 2000\n",
      "kp20k_valid2k ：1300 / 2000\n",
      "kp20k_valid2k ：1350 / 2000\n",
      "kp20k_valid2k ：1400 / 2000\n",
      "kp20k_valid2k ：1450 / 2000\n",
      "kp20k_valid2k ：1500 / 2000\n",
      "kp20k_valid2k ：1550 / 2000\n",
      "kp20k_valid2k ：1600 / 2000\n",
      "kp20k_valid2k ：1650 / 2000\n",
      "kp20k_valid2k ：1700 / 2000\n",
      "kp20k_valid2k ：1750 / 2000\n",
      "kp20k_valid2k ：1800 / 2000\n",
      "kp20k_valid2k ：1850 / 2000\n",
      "kp20k_valid2k ：1900 / 2000\n",
      "kp20k_valid2k ：1950 / 2000\n"
     ]
    }
   ],
   "source": [
    "filepath = ['kp20k_valid2k']#,'krapivin','duc',,'nus','inspec'\n",
    "\n",
    "for i in filepath:\n",
    "    get_pred_BART(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1626273717542,
     "user": {
      "displayName": "Diya A",
      "photoUrl": "",
      "userId": "03482944229402493818"
     },
     "user_tz": -480
    },
    "id": "I3Ym80VEdytd",
    "outputId": "695abb89-d17b-4ef5-9da0-a35ca5b248bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "list1 = [[\"grid\"], [\"service\", \"discovery\"], [\"uddi\"], [\"web\", \"services\"], \n",
    "[\"grid\", \"service\", \"discovery\"], [\"web\", \"services\", \"discovery\"], [\"web\", \"service\", \"discovery\"], \n",
    "[\"scalable\", \"grid\", \"service\", \"discovery\"], [\"autonomous\", \"control\"], [\"dude\", \"architecture\"], \n",
    "[\"uddi\", \"registries\"], [\"dude\"], [\"uddi\", \"registries\", \"thereby\", \"making\"], [\"uddi\", \"registries\", \"thereby\"], \n",
    "[\"web\", \"services\", \"control\"], [\"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\"], \n",
    "[\"web\", \"services\", \"registries\"], [\"uddi\", \"registries\", \"thereby\", \"making\", \"distribution\"], \n",
    "[\"uddi\", \"registries\", \"thereby\", \"making\", \"scaling\"], [\"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\", \"registries\"], \n",
    "[\"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\", \"more\"], [\"<unk>\"], \n",
    "[\"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\", \"more\", \"scaling\"], \n",
    "[\"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\", \"registries\", \"thereby\"],\n",
    "[\"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\", \"more\", \"robust\"],\n",
    "[\"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\", \"registries\", \"thereby\", \"making\"],\n",
    "[\"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\"],\n",
    "[\"scalable\", \"web\", \"services\"], [\"scalable\", \"web\"], [\"web\", \"service\"], [\"scalable\", \"web\", \"service\"], \n",
    "[\"scalable\", \"web\", \"services\", \"discovery\"], [\"scalable\", \"web\", \"service\", \"discovery\"],\n",
    "[\"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\", \"registries\"],\n",
    "[\"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\", \"more\"],\n",
    "[\"scalable\", \"web\", \"service\", \"discovery\", \"mechanisms\"],\n",
    "[\"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\", \"registries\", \"thereby\"],\n",
    "[\"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\", \"registries\", \"thereby\", \"making\"],\n",
    "[\"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\"],\n",
    "[\"<unk>\", \"<unk>\"], [\"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\", \"registries\"], \n",
    "[\"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\", \"registries\", \"thereby\", \"making\", \"uddi\", \"more\"],\n",
    "[\"web\"], [\"scalable\"], [\"scalable\", \"web\", \"service\", \"discovery\", \"based\"]]\n",
    "\n",
    "GTKP = [\"bamboo dht code\", \"uddi registry\", \"md\", \"distributed web service discovery architecture\", \n",
    "\"longest available prefix\",\"autonomous control\", \"discovery\", \"scalability issue\", \n",
    "\"dht based uddi registry hierarchy\", \"grid computing\", \"grid service discovery\", \n",
    "\"deployment issue\", \"uddi\", \"web service\", \"soft state\", \"query\", \"dht\", \"case insensitive search\", \n",
    "\"qos based service discovery\"]\n",
    "\n",
    "\"uddi\",\"grid service discovery\",[\"autonomous\", \"control\"],\"uddi\", \"registries\",\n",
    "\n",
    "\n",
    "\n",
    "[\"['grid computing', 'web service discovery', 'distributed hash tables', 'dht', 'uddi','scalability',\n",
    " 'grid service discovery architecture', 'autonomous control','spatial data mining', 'peer to peer', \n",
    " 'parallel computing and distributed systems', 'internet of things']\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (len(list1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3666,
     "status": "ok",
     "timestamp": 1641833252502,
     "user": {
      "displayName": "Diya A",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03482944229402493818"
     },
     "user_tz": -540
    },
    "id": "hnWA_hNHVil6",
    "outputId": "f7831db0-9661-40b9-b62d-e2ec2a43246a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[\"keyphrase extraction\", \"keyphrase generation\", \"deep learning\", \"condensed summaries of text\", \"bodied conversational agents\", \"cross encoder\", \"finiteness\", \"recall\", \"precision\", \"present and absent keyphrases\", \"semantic representation\", \"strong baselines\"]']\n"
     ]
    }
   ],
   "source": [
    "ARTICLE_TO_SUMMARIZE = '''\n",
    "Keyphrases are condensed summaries of text information, representing a document's theme and core idea. Keyphrase extraction approaches based on statistics and deep learning have received widespread attention. However, the extractive approaches have obvious limitations like keyphrases must occur in the document. However, a considerable amount of human-annotated keyphrases are absent keyphrases such that they are not occurring in the source document explicitly, which cannot be extracted by the extractive methods. Keyphrase generation is based on a sequence-to-sequence deep learning model, generating present and absent keyphrases from the semantic representation of the document. However, recall and precision of generated abstract keyphrases still need to be improved. We propose a two-step keyphrase generation model to solve these problems, which consists of a finetuned BART-based keyphrase generator, coupled with a BERT cross encoder which ranks generated keyphrases. Our model can generate absent keyphrases and shows superior results over the strong baselines in four widely used datasets.\n",
    "'''\n",
    "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], return_tensors='pt', padding=True, truncation=True,).to(device)\n",
    "\n",
    "\n",
    "# Generate keyphrases\n",
    "#model.config.min_length = 128\n",
    "summary_ids = model.generate(inputs['input_ids'], max_length = 256, min_length = 64, num_beams = 11).to(device)  #length_penalty = 3.0, top_k = 5,, no_repeat_ngram_size=3, early_stopping=True,, min_length = 64,\n",
    "bart_pred = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in summary_ids]\n",
    "print (bart_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NiA8RiJoMiG1"
   },
   "outputs": [],
   "source": [
    "key_Catseq = '''\n",
    "\t\t[5.3575]\t[impedance coupling] \t [2931, 1280] [correct!]\n",
    "\t\t[5.3575]\t[content targeted advertising] \t [368, 3326, 4019] [correct!]\n",
    "\t\t[6.7950]\t[on line advertising] \t [21, 387, 4019] \n",
    "\t\t[6.9837]\t[search based advertising] \t [145, 27, 4019] \n",
    "\t\t[8.6871]\t[web page] \t [149, 2019] \n",
    "\t\t[22.1644]\t[impedance coupling strategy] \t [2931, 1280, 365] \n",
    "\t\t[27.6107]\t[content targeted advertising can] \t [368, 3326, 4019, 32] \n",
    "\t\t[29.4837]\t[vocabulary impedance] \t [3787, 2931] \n",
    "\t\t[29.7378]\t[computer science] \t [176, 800] \n",
    "\t\t[29.8833]\t[impedance] \t [2931] \n",
    "\t\t[42.5659]\t[content] \t [368] \n",
    "\t\t[42.7403]\t[web] \t [149] [correct!]\n",
    "\t\t[43.8703]\t[text] \t [564] \n",
    "\t\t[44.0307]\t[vocabulary] \t [3787] \n",
    "\t\t[44.0490]\t[computer] \t [176] \n",
    "    '''\n",
    "\n",
    "\n",
    "Key_BART_Beam9 = ['algorithms', 'design', 'experimentation','measurement', 'information retrieval',\n",
    "                  'content targeted advertising', 'impedance coupling', 'web search', 'tagging', \n",
    "                  'association','semantic', 'access', 'text', 'algorithm', 'informal', 'user','search']\n",
    "\n",
    "\n",
    "key_BART_Beam7 = '''\n",
    "['algorithms', 'experimentation','measurement', 'human factors', \n",
    "'content targeted advertising', 'impedance coupling', 'web page matching', \n",
    "'information retrieval', 'navigation', 'text mining','search', 'tagging', \n",
    "'algorithm', 'paper', 'informal', 'user']\n",
    "'''\n",
    "\n",
    "key_BART_Beam6 = '''\n",
    "[\\'algorithms\\', \\'experimentation\\',\\'measurement\\', \\'human factors\\', \n",
    "\\'content targeted advertising\\', \\'impedance coupling\\', \\'web search\\', \n",
    "\\'information retrieval\\', \\'text mining\\',\\'search based advertising\\', \n",
    "\"user\\'s business\", \\'web page\\', \\'tagging\\', \\'contention\\',\\'strategies\\', \n",
    "\\'optimality\\', \\'association\\',\\'semantic\\',\\'matching\\', \n",
    "\\'data\\', \\'algorithm\\', \\'effect\\', \\'query\\']\n",
    "'''\n",
    "\n",
    "Key_BART_Beam5 = '''\n",
    "[\\'content targeted advertising\\', \\'impedance coupling\\', \\'web search\\', \n",
    "\\'information retrieval\\',\\'search based advertising\\', \"user\\'s business\", \n",
    "\\'web page matching\\', \\'text mining\\',\\'semantic web\\', \\'navigation\\', \n",
    "\\'association\\', \\'contention\\',\\'strategies\\',\\'method\\', \\'accuracy\\', \\'users\\', \n",
    "\\'optimality\\',\\'matching\\',\\'retrieval\\', \\'data\\', \\'algorithm\\', \\'query\\']\n",
    "'''\n",
    "\n",
    "Key_BART_Beam3 = '''\n",
    "[\\'content targeted advertising\\', \\'impedance coupling\\', \\'web search\\', \n",
    "\\'information retrieval\\',\\'search based advertising\\', \"user\\'s business\", \n",
    "\\'web page matching\\', \\'text mining\\', \\'data mining\\', \"aak\\'s algorithm\", \n",
    "\\'information extraction\\', \\'query processing\\', \\'navigation\\', \\'algorithms\\']\n",
    "'''\n",
    "Key_BART_No_Beam = '''\n",
    "[\\'content targeted advertising\\', \\'impedance coupling\\', \\'web search\\'\n",
    "]advertisement\\']advertisementadvertisement\\',\\'search based advertising\\', \n",
    "\"user\\'s business\"]advertisementadvertisement\\']... \n",
    "\\'advertisement advertisement\\', \\'advertisement matching\\', \n",
    "\\'content based advertising\\']. \\'web page matching\\', \n",
    "\"adman\\'s keyword\", \\'web site matching\\',\n",
    "'''\n",
    "\n",
    "\n",
    "keywords = [\"advertisements\", \"triggering page\", \"bayesian networks\", \"advertising\", \"matching\", \n",
    "            \"knn\", \"web\", \"content targeted advertising\", \"impedance coupling\"]\n",
    "\n",
    "B10_len200 = \n",
    "['[\\'algorithms\\', \\'experimentation\\',\\'measurement\\', \\'performance\\', \\'theory\\', \n",
    "\\'content targeted advertising\\', \\'impedance coupling\\', \\'information retrieval\\', \\'web search\\',\n",
    "\\'semantic web\\', \\'wordnet\\',\\'search engine\\', \\'user navigation\\', \\'on line advertising\\', \"user\\'s business\", \n",
    "\\'web page\\',\\'vectorization\\', \\'association\\', \\'text\\', \\'algorithm\\', \\'effect\\', \\'evaluation\\', \\'query\\', \n",
    "\\'coupling\\',\\'scheme\\', \\'ad hoc\\', \\'combinational\\', \\'advertising\\', \\'pagerank\\', \\'all\\',\\'semi supervised learning\\', \n",
    "\\'classification\\',\\'matching\\', \\'clustering\\', \\'data mining\\', \\'dynamic programming\\', \\'decision trees\\', \\'learning\\', \n",
    "\\'internet\\', \\'interactive\\',\\'multimedia\\', \\'distributed\\', \\'demonstrate\\', \\'complexity\\', \\'utility\\', \\'profiles\\', \\\n",
    "\\'audience\\', \\'documentation\\', \\'information systems\\', \\'tools\\',\\'mobile computing\\',\\'resource\\', \\'business\\',\n",
    "\\'\\'information\\',\\'\\'internet advertising\\'] \\'web\\',\\'social networks\\', \\'digital\\'ads\\', \\'computer science\\', \\'']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "BL1028 = '''\n",
    "[\"precise\", \"yield\", \"contention\", \"vectorization\", \"method\", \"strategies\", \"accuracy\", \n",
    "\"semantic\", \"association\", \"web page\", \"matching\", \"text\", \"operability\", \"algorithm\", \"access\", \n",
    "\"effect\", \"informal\", \"evaluation\", \"user\", \"search\", \"couples\", \"targeted advertising\", \"ad hoc networks\", \n",
    "\"information retrieval\", \"on line advertisement\", \"contextual advertising\"]advertisement\", \"advertisement\", \n",
    "\"keyword matching\", \"advertisement correlation\", \"advertisement conflict resolution\", \"content targeted advertising\", \"<digit> <digit>\"]']\n",
    "\n",
    "'''\n",
    "['[\"precise\", \"yield\", \"contention\", \"vectorization\", \"method\", \"accuracy\", \"semantic\", \"association\"\n",
    ", \"web page\", \"matching\", \"text\", \"operability\", \"algorithm\", \"access\", \"effect\", \"informal\", \n",
    " \"evaluation\", \"user\", \"search\", \"couples\", \"business\"]']\n",
    "\n",
    "\n",
    "\n",
    "BL_512_128 = '''\n",
    "['[\\'algorithms\\', \\'experimentation\\',\\'measurement\\', \\'performance\\', \\'content targeted advertising\\', \n",
    "\\'impedance coupling\\', \\'on line advertising\\', \"user\\'s navigation\",\\'semantic web\\', \\'web page\\', \n",
    "\\'text\\', \\'attributes\\', \\'account\\', \\'algorithm\\', \\'effect\\', \\'evaluation\\',\\'search\\', \\'coupling\\',\n",
    "\\'vectorization\\', \\'association\\',\\'matching\\',\\'scheme\\', \\'business\\', \\'information retrieval\\', \\'query\\',\n",
    " \\'ad hoc\\', \\'combinational\\', \\'user\\', \\'cluster\\', \\'advertising\\', \\'pagerank']\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "B20_len256 = '''\n",
    "[\"['contention','method', 'accuracy','semantic', 'web page', 'computation','matching', \n",
    "'text', 'account', 'access', 'algorithm', 'informal', 'effect', 'evaluation', 'user','search', \n",
    "'couples', 'association', 'online advertising', 'advertising','scheme', 'information retrieval', \n",
    "'query', 'content targeted', 'pagerank','vectorization', 'business', 'ad hoc', 'ranking', 'integrability', \n",
    "'classifiation', 'robotics', 'circ','semi supervised learning', 'natural language processing', \n",
    "'world wide web','syntactic similarity', 'clustering', 'internet', 'tools', 'interactive systems', \n",
    "'computer science', 'data', 'profiles', 'utility', 'complexity','systems', 'transparency', \n",
    "'demonstrate',''web','resource', 'distributed networks','software','ads',''utimedia', 'privacy','mobile', \n",
    "'un''utilities', 'intelligent interfaces','advertisements','social advertisement', 'virtual '\"]\n",
    "'''\n",
    "\n",
    "\n",
    "B30_len512 = '''\n",
    "['[\\'contention\\',\\'method\\', \\'accuracy\\',\\'semantic\\', \\'web page\\', \\'association\\', \\'computation\\',\n",
    "\\'matching\\', \\'text\\', \\'account\\', \\'algorithm\\', \\'informal\\', \\'evaluation\\', \\'user\\',\\'search\\', \\'couples\\',\n",
    "\\'vectorization\\', \\'business\\', \\'advertising\\', \\'effect\\', \\'information retrieval\\',\\'scheme\\', \n",
    "\\'content targeted advertising\\', \\'world wide web\\', \\'query\\', \\'pagerank\\', \\'embedding\\', \n",
    "\\'integrability\\', \\'ad hoc\\', \\'robotics\\', \\'programming and algorithm theory\\',\\'\\'query optimization\\', \n",
    "\\'classification\\', \\'clustering\\', \\'feature extraction\\', \\'data mining\\', \\'ranking\\', \\'tools\\', \\'profiles\\',\n",
    " \\'demonstrate\\', \\'internet\\',\\'segmentation techniques\\',\\'similarity\\', \\'utility\\', \\'interactive systems\\', \n",
    " \\'all\\', \\'documentation tools\\',\\'social networks\\',\\'resource\\', \\'computer science\\', \\'electronic commerce\\', \n",
    " \\'distributed applications\\', \\'contin advertisements\\',\\'multimedia\\',\\'multi user interfaces\\',\\'systems\\',\\'\\'user interface\\', \n",
    " \\'privacy\\',\\'mobile computing\\', \\'audience\\', \\'transactions\\',\\'software\\',\\'ads\\', \\'un\\'\\'c \" advertisement\\',\n",
    " \\'advertisements\\', \" ad enterprises\\', \\'trust ads\\'ad interfaces\\', \" ads ad\\'advertisements\\'] advertisements\\'ads\\',\n",
    " \\'\\' advertising\\'advertisement\\'interfaces\\'] \\'\\', advertising\\'] interfaces and advertising advertisements\\', advertisements\\',\\', \n",
    " \\'\\',\\'interfaces\\', advertising interfaces\\', ad ads\\', ads\\', advertisement\\', advertisement ads\\', advertisers\\', \n",
    " \\'imedia advertisements\\', advertisement advertisements\\', ad advertisements\\', advertising advertisements\\'] \n",
    " \\'internet advertising\\'advertisements\\']\\',\\'keywords\\', \\'environment\\',\\'advertisement\\'\\'internet advertisements\\'\\' \n",
    " \\'web advertising\\'\\' advertising ads\\', ad\\'\\' advertisers\\', advertisement\\'ads\\']\\'advertisements \\'internet ads\\', \n",
    " \"\\', \\'\", \\'internet\\'interfaces\\'\\'document\\'\\' advertisements\\'advertisement ad\\'ads\\'\\'advertising\\'\\' ad\\',\n",
    " \\'advertising\\'advertisers\\'\\' advertisement\\', \" \\'internet advertisement\\'advertising ad\\'ad\\'advertisement advertisements\\\n",
    " 'ads ads\\'ads ad\\'advertisements ad\\'advertisers\\']\\'\\'business\\'\\' ads and \\'internet ad\\'advertising advertising\\'ads advertisements\\',\n",
    "  ads\\'advertisement advertising\\'ad ads\\'ad advertisements\\'advertisements and\\'\\' commercials\\',\\'ad']\n",
    "'''\n",
    "\n",
    "B50_len512 = '''['[\\'contention\\',\\'strategies\\', \\'accuracy\\',\\'semantic\\', \\'web page\\', \\'computation\\',\n",
    "\\'matching\\', \\'addressing\\', \\'account\\', \\'access\\', \\'algorithm\\', \\'informal\\', \\'evaluation\\', \\'user\\',\n",
    "\\'search\\', \\'couples\\',\\'vectorization\\', \\'association\\', \\'text\\', \\'advertising\\', \\'effect\\',\\'scheme\\', \n",
    "\\'world wide web\\', \\'information retrieval\\', \\'content targeted advertising\\', \\'pagerank\\', \\'classifiation\\', \n",
    "\\'ad hoc\\', \\'ranking\\', \\'query\\', \\'integrability\\', \\'all\\',\\'semi supervised learning\\', \n",
    "\\'interactive learning environments\\', \\'clustering\\', \\'unsupervised learning algorithms\\', \\'learning\\', \n",
    "\\'data mining\\', \\'computer science\\', \\'profiles\\', \\'collaborative filtering\\', \\'tools\\', \\'electronic commerce\\',\n",
    "\\'multimedia\\', \\'internet\\', \\'privacy\\', \\'utility\\', \\'embedded systems\\', \\'business\\',\\'\\'web\\', \\'contextual information\\',\n",
    "\\'\\'user interfaces\\',\\'advertisements\\',\\'social networks\\',\\'systems\\', \\'demonstrate\\', \\'contin\\'\\'web ads\\', \\'transactions\\',\n",
    "\\'keywords\\',\\'multi channels\\', \\'dist\\'\\' advertising advertisement\\', \\'customisation\\', \\'global ads \" advertisements\\', \n",
    "\" ads\\'ad advertisements\\'] advertising\\', advertisements\\'ads\\'] interfaces advertising\\'advertisement\\'interfaces\\']\\']\n",
    " ad\\'commercials\\'interfaces\\', ad ad advertisements\\', advertisement advertisements\\', ads interfaces\\'advertisers\\', \\'& ad interfaces\\', \n",
    " \" ad advertising ads\\', ads\\', advertising advertisements\\', advertisements\\', \\'\\', \\'imedia advertisements\\',\\',\\'commercials\\', \n",
    " \\'commercial ad\\'advertisements ad\\'\\'internet advertisements\\', advertising\\'\\' advertising advertising advertisements\\'] \n",
    " \\'internet advertising \\'internet ads\\', advertisements\\'\\'\\'ads\\', advertisement\\'\\' advertisers\\', advertisement ads\\', ad\\',\n",
    " \\'enterprises\\', \\'document\\',\\'advertisement\\'advertisements\\'ads\\'\\'advertising ads\\', \"\\'\\' advertisement\\', ad \\'internet\\'\n",
    " \\' advertising\\'advertisers\\'\\' ad\\'ads\\']\\'\\' advertisements and\\'\\'business\\'\\'\", \\'internet advertisement \\'internet ad\\\n",
    " 'advertisement advertisements\\'advertisement ads\\'ads ad\\'advertisements ad ads\\'advertisement ad\\'ad ads\\', advertising ad\\'advertising\\',\n",
    "  ads\\'advertising ads\\'ad advertisements\\'advertising advertisements\\'ad advertising\\'advertisement advertising \\'']\n",
    "\n",
    "'''\n",
    "\n",
    "B1_len512 = '''\n",
    "['[\\'web advertising\\', \\'content targeted advertising\\', \"user\\'s information\"] \\'impedance coupling\\'] \\'web page matching\\'] \n",
    "\\'wordnet\\'] \\'information retrieval\\'] \\'query optimization\\', \\'information extraction\\'] \\'text mining\\', \n",
    "\\'text classification\\', \\'web search\\', \\'query processing\\', \\'user interface\\'] \\'user navigation\\',\n",
    " \\'on line advertising\\', \\',\\'vectorization\\',\\'strategies\\', \\'accuracy\\',\\'semantic\\',\\'matching\\', \\'algorithm\\', \n",
    " \\'effect\\', \\'couples\\', \\'advertising\\',\\'search\\', \\'internet\\'] \\'effectiveness\\'] \\'algorithms\\', \\'world\\'\\'webpage\\', \n",
    " \\'ad hoc\\'] \\'internet\\',\\'\\'\\', \"\\'\\' \\'web\\'\"#\\'advertisement \\'\"]\\'\" \"………………………………>>>>>>>>\\'ad \\'……………………\\'advertisement\\'\\'\n",
    "  \\'all\\'\"# ad\\'ads\\', \\'ape\\'advertisements\\',\\'\\'ape ad \"\\'advertisers\\', advertisement\\', \\'all\\'ubiquitous \n",
    "  \\'ube\\'commercials\\', ad hoc \\'ould \\'allould advertisements\\'multimedia \\'internet\\'clutter \\\n",
    "  'document\\'advertising\\'interfaces\\', \\'document \\'& \\'az\\'autop\\'advertisers\\', \\'edge\\'ads\\',\\'advertisement\\', \n",
    "  advertisements\\']\\'advertisements\\', ads \\'\", \\'& ads \\'al \\'edge\\'emergency\\'ad\\'\\'document ad \\'document\\', \n",
    "  advertisement ad\\'advertisement ad ad ad\\'advertisers\\'internet\\'enterprise \\'document\\'\\' ads\\'\\'internet ad\\'ads ad \\'\",\n",
    "  \\'\\' advertisement ads \\'\\', \\'\", \\'\", \"\\', ad \\'\\', \"\\'\\'\", ad \\'[\\'\\'\\' advertisers\\']\\'\\' advertisements\\'\\' ad ad ad ads\\'\n",
    "  ads\", \\'internet advertisement\\'ads\\']\\'&\\'\" ad\\',\\'ads ads\\'advertisement advertisement \\'internet ads\\'ad ads\\', \n",
    "  \"\\',\\'advertisements ad\\'ad\\', \"\",\\'ads advertisement\\'advertisement advertisements\\'ads advertisements\\'advertisement advertising\\'\n",
    "  \\'advertising\\'\\' advertising\\'ads advertising \\'\", ads \\'\",\\',\\'ad\", \\'\", advertisement\\'ad advertisement \\'\",\", \\'document ads \n",
    "  \\'internet advertising\\'ad advertising\\'advertisement\",\\'advertisement \"\\'ads \" \\'internet advertisements\\'ad advertisements \\'\", \n",
    "  advertisements \\'internet\", \\'c\\'\\' \"\\'ad']\n",
    "'''\n",
    "\n",
    "B9_len512 = '''\n",
    "['[\\'algorithms\\', \\'experimentation\\',\\'measurement\\', \\'performance\\', \\'theory\\', \\'content targeted advertising\\', \n",
    "\\'impedance coupling\\', \\'information retrieval\\',\\'semantic web\\', \\'web search\\', \\'on line advertising\\', \n",
    "\"user\\'s business\", \\'web page\\', \\'wordnet\\',\\'search engine\\', \\'user navigation\\', \\'query\\',\\'vectorization\\',\n",
    "\\'strategies\\', \\'accuracy\\', \\'optimality\\', \\'text\\', \\'algorithm\\', \\'effect\\', \\'coupling\\',\\'scheme\\', \\'advertising\\', \n",
    "\\'ranking\\', \\'combinational\\', \\'ad hoc\\']\\'search\\', \\'pagerank\\', \\'classification\\', \\'cluster\\', \\'data mining\\', \n",
    "\\'tools\\', \\'internet\\',\\'searchers\\',\\'\\'query languages\\',\\'similarity\\',\\'resource\\',\\'ads\\', \\'demonstrate\\', \\'utility\\', \n",
    "\\'audit\\', \\'profiles\\', \\'all\\', \\'online\\', \\'business\\', \\'dynamic programming\\', \\'documentations\\', \\'database\\', \n",
    "\\'computer science\\', \\'interactive\\', \\'natural \\'information systems\\', \\'contin information\\', \\'distributed systems\\',\n",
    "\\'query\\'ads\\', advertisements\\',\\'advertisements\\', \" ad advertisements ads\\'ad\\'advertisements\\'] \" advertisements\\'advertisement\\',\n",
    "\\'interfaces\\', ads\\'] advertising\\'] ads ads advertising\\'\\'\\'ads ad ad advertisement \\'\\'] advertisements ad ad advertisements\\', \n",
    "advertisements advertisement advertisements\\', advertisers\\', advertisements advertisements\\', ad \\'\\', \\'\\',\\', ad ads\\', \" \n",
    "interfaces\\'advertising advertisements\\', advertisement ads\\', advertisements\\',\\',\\'strate\\'advertisers\\', \\'imedia advertisements \n",
    "\\'internet \\'\\', advertisement\\', ads\\', advertisement\\'advertisements ad\\'enterprises\\', \\'circ\\'advertisement\\'\\' advertisements ads\\',\n",
    "\\'advertisements\\'\\'\\'advertisers\\', advertisement advertisements\\'advertisement\\', advertisement ad\\'\\' advertisement advertisements\\'] \n",
    "ad\\',\\'advertisement ads \\'document\\', \\'\",\\'\\'internet advertisements\\', ads ads\\', ad ads\\'\\' ads \\'\\', \"\\', \\'\n",
    "[\\'internet advertising\\'advertisements\\'] \\'internet advertisement\\'ads\\']\\'\\' ad\\'advertisers\\'\\' advertising\\'\\'advertising\\'\\' \n",
    "\"\\'\\'document\\'\\'\\', ads\\'ads ads\\'advertisement advertising\\'ads advertisements\\', advertising ads\\',\\', ad \\'internet ads\\', \n",
    "advertising \\'internet advertising advertisements\\'ads ad\\'ads advertising\\'advertisement advertisement\\'advertising\\',']\n",
    "'''\n",
    "\n",
    "large-bart = ['algorithms', 'experimentation','measurement', 'performance', 'content targeted advertising', \n",
    "              'impedance coupling','semantic web', 'information retrieval', 'web search', 'text mining', \n",
    "              'wordnet','search engine', 'user interface', 'on line advertising']\n",
    "\n",
    "Key_BART_Beam9 = ['algorithms', 'design', 'experimentation','measurement', 'information retrieval',\n",
    "                  'content targeted advertising', 'impedance coupling', 'web search', 'tagging', \n",
    "                  'association','semantic', 'access', 'text', 'algorithm', 'informal', 'user','search']\n",
    "\n",
    "B9_len256 = '''\n",
    "['</s>[\\'algorithms\\', \\'experimentation\\',\\'measurement\\', \\'performance\\', \\'theory\\', \n",
    "\\'content targeted advertising\\', \\'impedance coupling\\', \\'information retrieval\\',\\'semantic web\\', \n",
    "\\'web search\\', \\'on line advertising\\', \"user\\'s business\", \\'web page\\', \\'wordnet\\',\\'search engine\\', \n",
    "\\'user navigation\\', \\'query\\',\\'vectorization\\',\\'strategies\\', \\'accuracy\\', \\'optimality\\', \n",
    "\\'text\\', \\'algorithm\\', \\'effect\\', \\'coupling\\',\\'scheme\\', \\'advertising\\', \\'ranking\\', \n",
    "\\'combinational\\', \\'ad hoc\\']\\'search\\', \\'pagerank\\', \\'classification\\', \\'cluster\\', \\'data mining\\', \n",
    "\\'tools\\', \\'internet\\',\\'searchers\\',\\'\\'query languages\\',\\'similarity\\',\\'resource\\',\\'ads\\', \\'demonstrate\\', \n",
    "\\'utility\\', \\'audit\\', \\'profiles\\', \\'all<s>\\', \\'online\\', \\'business\\', \\'dynamic programming\\',\n",
    " \\'documentations\\', \\'database\\', \\'computer science\\', \\'interactive\\', \\'natural<s> \\'information systems\\',\n",
    "  \\'contin<s><s><s> information\\', \\'distributed systems\\']<s> \\'query<s><s> \\'<s><s> ads\\',<s><s> advertisements\\', \\'<s></s>']\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMi4woMnUIPKaBuKGqb7kue",
   "machine_shape": "hm",
   "mount_file_id": "1MWp9buG6u-_BOsjmB01sLN0OjEA_Ui0Y",
   "name": "Bart generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
