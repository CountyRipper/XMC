{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/delab/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2022-10-11 11:10:46,628 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda\n",
      "[nltk_data] Downloading package stopwords to /home/delab/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from premethod import *\n",
    "from pegasus_fine_tune import *\n",
    "from generate_pegasus import *\n",
    "from combine import *\n",
    "from rank import rank\n",
    "from rank_training import rank_train\n",
    "from p_at_1 import p_at_k\n",
    "from keybart_finetune import *\n",
    "from keybart_generate import get_pred_Keybart\n",
    "from bart_finetune import fine_tune_bart\n",
    "from bart_generate import get_pred_bart\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = ['../dataset/EUR-Lex/','../dataset/Wiki500K/']\n",
    "k_fold = [0,1,2,3,4]\n",
    "tasks = ['test','train','valid']\n",
    "models = {'pega':0,'bart':1,'kb':0}\n",
    "gener = \"generate_result/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname=[datadir[1]+tasks[0]+\"_labels20.txt\",datadir[1]+tasks[1]+\"_labels20.txt\"]\n",
    "get_all_labels(dataname,datadir[1]+\"all_labels20.txt\")\n",
    "get_all_stemlabels(datadir[1]+'all_labels20.txt',datadir[1]+'all_stemlabels20.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pegasus\n",
    "if models['pega']:\n",
    "        Pegasus_fine_tune(datadir[0],tasks[1]+'_finetune.json',tasks[0]+'_finetune.json',\"pegasus_save\",\"pegasus_check\")\n",
    "        for i in range(2):\n",
    "            get_pred_Pegasus_fast(datadir[1],gener+tasks[i]+\"_pred.txt\",tasks[i]+\"_finetune.json\",\"pegasus_save\")\n",
    "            get_combine_list(datadir[1],gener+tasks[i]+\"_pred.txt\",\"all_stemlabels.txt\",tasks[i]+\"_combine_labels.txt\")\n",
    "        rank_train(datadir[1],tasks[1]+\"_texts.txt\",tasks[1]+\"_combine_labels.txt\",tasks[1]+\"_labels_stem.txt\",\"cr_en\")\n",
    "        rank(datadir[1],tasks[0]+\"_texts.txt\",tasks[0]+\"_combine_labels.txt\",\"cr_en\",tasks[0]+\"_ranked_labels.txt\")\n",
    "        res = p_at_k(datadir[1],tasks[0]+\"_labels_stem.txt\",tasks[0]+\"_ranked_labels.txt\",datadir[1]+\"res_pega.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../dataset/Wiki500K/bart_save/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ../dataset/Wiki500K/bart_save/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: ../dataset/Wiki500K/generate_result/test_pred_ba.txt\n",
      "src_data: ../dataset/Wiki500K/test_finetune20.json\n",
      "model_path: ../dataset/Wiki500K/bart_save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at ../dataset/Wiki500K/bart_save.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "Didn't find file ../dataset/Wiki500K/bart_save/added_tokens.json. We won't load it.\n",
      "loading file ../dataset/Wiki500K/bart_save/vocab.json\n",
      "loading file ../dataset/Wiki500K/bart_save/merges.txt\n",
      "loading file None\n",
      "loading file ../dataset/Wiki500K/bart_save/special_tokens_map.json\n",
      "loading file ../dataset/Wiki500K/bart_save/tokenizer_config.json\n",
      "tokenizer config file saved in ../dataset/Wiki500K/bart_tokenizer/tokenizer_config.json\n",
      "Special tokens file saved in ../dataset/Wiki500K/bart_tokenizer/special_tokens_map.json\n",
      "  0%|          | 1/39188 [00:00<8:44:11,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autism spectrum, Disability rights, Neurodevelopmental disorders, Syndromes, Transcription factor deficiencies, Tropical and subtropicaluddin II and III, Vaccination-related cutaneous conditions, World Health Organization essential medicines']======x80x93West Virginia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 99/39188 [00:53<5:53:04,  1.85it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/delab/Hard/XMC/src/process.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/delab/Hard/XMC/src/process.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m models[\u001b[39m'\u001b[39m\u001b[39mbart\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/delab/Hard/XMC/src/process.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m         \u001b[39m#fine_tune_bart(datadir[1],tasks[1]+'_finetune20.json',tasks[0]+'_finetune20.json','bart_save','bart_check')\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/delab/Hard/XMC/src/process.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/delab/Hard/XMC/src/process.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m             get_pred_bart(datadir[\u001b[39m1\u001b[39;49m],gener\u001b[39m+\u001b[39;49mtasks[i]\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m_pred_ba.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m,tasks[i]\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m_finetune20.json\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mbart_save\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/delab/Hard/XMC/src/process.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m             bart_clean(datadir[\u001b[39m1\u001b[39m]\u001b[39m+\u001b[39mgener\u001b[39m+\u001b[39mtasks[i]\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_pred_ba.txt\u001b[39m\u001b[39m\"\u001b[39m,datadir[\u001b[39m1\u001b[39m]\u001b[39m+\u001b[39mgener\u001b[39m+\u001b[39mtasks[i]\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_pred_ba_c.txt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/delab/Hard/XMC/src/process.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m             get_combine_list(datadir[\u001b[39m1\u001b[39m],gener\u001b[39m+\u001b[39mtasks[i]\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_pred_ba_c.txt\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mall_stemlabels20.txt\u001b[39m\u001b[39m\"\u001b[39m,gener\u001b[39m+\u001b[39mtasks[i]\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_combine_labels_ba.txt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Hard/XMC/src/bart_generate.py:46\u001b[0m, in \u001b[0;36mget_pred_bart\u001b[0;34m(dir, output_dir, src_dataname, model_path)\u001b[0m\n\u001b[1;32m     44\u001b[0m dic \u001b[39m=\u001b[39m data[i]\n\u001b[1;32m     45\u001b[0m src_value \u001b[39m=\u001b[39m dic[\u001b[39m\"\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 46\u001b[0m tmp_result \u001b[39m=\u001b[39m single_pred(model,tokenizer,src_value)\n\u001b[1;32m     47\u001b[0m dic[\u001b[39m\"\u001b[39m\u001b[39mpred\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m tmp_result\u001b[39m.\u001b[39mreplace (\u001b[39m'\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     48\u001b[0m res_labels\u001b[39m=\u001b[39m[]\n",
      "File \u001b[0;32m~/Hard/XMC/src/premethod.py:172\u001b[0m, in \u001b[0;36msingle_pred\u001b[0;34m(model, tokenizer, document_src)\u001b[0m\n\u001b[1;32m    170\u001b[0m   inputs \u001b[39m=\u001b[39m tokenizer([document_src], return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mto(device)\u001b[39m#, padding=True\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# Generate Summary\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m   summary_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(inputs[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m],max_length \u001b[39m=\u001b[39;49m \u001b[39m256\u001b[39;49m,min_length \u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,num_beams \u001b[39m=\u001b[39;49m \u001b[39m7\u001b[39;49m)\u001b[39m.\u001b[39mto(device)  \u001b[39m#length_penalty = 3.0  top_k = 5\u001b[39;00m\n\u001b[1;32m    173\u001b[0m   pre_result\u001b[39m=\u001b[39m[]\n\u001b[1;32m    174\u001b[0m   \u001b[39mfor\u001b[39;00m g  \u001b[39min\u001b[39;00m summary_ids:\n",
      "File \u001b[0;32m~/.conda/envs/ddd/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/ddd/lib/python3.8/site-packages/transformers/generation_utils.py:1315\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[1;32m   1311\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1312\u001b[0m         input_ids, expand_size\u001b[39m=\u001b[39mnum_beams, is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs\n\u001b[1;32m   1313\u001b[0m     )\n\u001b[1;32m   1314\u001b[0m     \u001b[39m# 12. run beam search\u001b[39;00m\n\u001b[0;32m-> 1315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   1316\u001b[0m         input_ids,\n\u001b[1;32m   1317\u001b[0m         beam_scorer,\n\u001b[1;32m   1318\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1319\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1320\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mpad_token_id,\n\u001b[1;32m   1321\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49meos_token_id,\n\u001b[1;32m   1322\u001b[0m         output_scores\u001b[39m=\u001b[39;49moutput_scores,\n\u001b[1;32m   1323\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1324\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1325\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1326\u001b[0m     )\n\u001b[1;32m   1328\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1329\u001b[0m     \u001b[39m# 10. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(\n\u001b[1;32m   1331\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k, top_p\u001b[39m=\u001b[39mtop_p, typical_p\u001b[39m=\u001b[39mtypical_p, temperature\u001b[39m=\u001b[39mtemperature, num_beams\u001b[39m=\u001b[39mnum_beams\n\u001b[1;32m   1332\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/ddd/lib/python3.8/site-packages/transformers/generation_utils.py:2177\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2172\u001b[0m next_token_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madjust_logits_during_generation(next_token_logits, cur_len\u001b[39m=\u001b[39mcur_len)\n\u001b[1;32m   2173\u001b[0m next_token_scores \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mlog_softmax(\n\u001b[1;32m   2174\u001b[0m     next_token_logits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m   2175\u001b[0m )  \u001b[39m# (batch_size * num_beams, vocab_size)\u001b[39;00m\n\u001b[0;32m-> 2177\u001b[0m next_token_scores_processed \u001b[39m=\u001b[39m logits_processor(input_ids, next_token_scores)\n\u001b[1;32m   2178\u001b[0m next_token_scores \u001b[39m=\u001b[39m next_token_scores_processed \u001b[39m+\u001b[39m beam_scores[:, \u001b[39mNone\u001b[39;00m]\u001b[39m.\u001b[39mexpand_as(next_token_scores)\n\u001b[1;32m   2180\u001b[0m \u001b[39m# Store scores, attentions and hidden_states when required\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ddd/lib/python3.8/site-packages/transformers/generation_logits_process.py:92\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m         scores \u001b[39m=\u001b[39m processor(input_ids, scores, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     91\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m         scores \u001b[39m=\u001b[39m processor(input_ids, scores)\n\u001b[1;32m     93\u001b[0m \u001b[39mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m~/.conda/envs/ddd/lib/python3.8/site-packages/transformers/generation_logits_process.py:333\u001b[0m, in \u001b[0;36mNoRepeatNGramLogitsProcessor.__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m    330\u001b[0m banned_batch_tokens \u001b[39m=\u001b[39m _calc_banned_ngram_tokens(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngram_size, input_ids, num_batch_hypotheses, cur_len)\n\u001b[1;32m    332\u001b[0m \u001b[39mfor\u001b[39;00m i, banned_tokens \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(banned_batch_tokens):\n\u001b[0;32m--> 333\u001b[0m     scores[i, banned_tokens] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    335\u001b[0m \u001b[39mreturn\u001b[39;00m scores\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if models['bart']:\n",
    "        #fine_tune_bart(datadir[1],tasks[1]+'_finetune20.json',tasks[0]+'_finetune20.json','bart_save','bart_check')\n",
    "        for i in range(2):\n",
    "            get_pred_bart(datadir[1],gener+tasks[i]+\"_pred_ba.txt\",tasks[i]+\"_finetune20.json\",\"bart_save\")\n",
    "            bart_clean(datadir[1]+gener+tasks[i]+\"_pred_ba.txt\",datadir[1]+gener+tasks[i]+\"_pred_ba_c.txt\")\n",
    "            get_combine_list(datadir[1],gener+tasks[i]+\"_pred_ba.txt\",\"all_labels20.txt\",gener+tasks[i]+\"_combine_labels_ba.txt\")\n",
    "        rank_train(datadir[1],tasks[1]+\"_texts.txt\",gener+tasks[1]+\"_combine_labels_ba.txt\",tasks[1]+\"_labels20.txt\",\"cr_en_ba\")\n",
    "        rank(datadir[1],tasks[0]+\"_texts20.txt\",gener+tasks[0]+\"_combine_labels_ba.txt\",\"cr_en_ba\",gener+tasks[0]+\"_ranked_labels_ba.txt\")\n",
    "        res = p_at_k(datadir[1],tasks[0]+\"_labels20.txt\",gener+tasks[0]+\"_ranked_labels_ba.txt\",datadir[1]+\"ba_res.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if models['kb']:    \n",
    "        #kb_fine_tune(datadir[0],\"kb_save\",\"kb_check\")\n",
    "        fine_tune_keybart(datadir[1],tasks[1]+'_finetune.json',tasks[0]+'_finetune.json','keybart_save','keybart_test')\n",
    "        for i in range(2):\n",
    "            get_pred_Keybart(datadir[0],gener+tasks[i]+\"_pred_kb.txt\",tasks[i]+\"_finetune.json\",\"keybart_save\")\n",
    "            bart_clean(datadir[0]+gener+tasks[i]+\"_pred_kb.txt\",datadir[0]+gener+tasks[i]+\"_pred_kb_c.txt\")\n",
    "            get_combine_list(datadir[0],gener+tasks[i]+\"_pred_kb_c.txt\",\"all_labels_sterm.txt\",gener+tasks[i]+\"_combine_labels_kb.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K则验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in k_fold:\n",
    "    k_dir = \"/K_fold/\"+\"K_\"+str(j)+\"/\"\n",
    "    Pegasus_fine_tune(datadir[0]+k_dir,tasks[1]+'.json',tasks[0]+'.json',\"pegasus_save\",\"pegasus_check\")\n",
    "    for i in range(2):\n",
    "        get_pred_Pegasus(datadir[0]+k_dir,tasks[i]+\"_pred.txt\",tasks[i]+\".json\",\"pegasus_save\")\n",
    "        get_combine_list(datadir[0]+k_dir,tasks[i]+\"_pred.txt\",\"all_stemlabels.txt\",tasks[i]+\"_combine_labels.txt\")\n",
    "    rank_train(datadir[0]+k_dir,tasks[1]+\"_texts.txt\",tasks[1]+\"_combine_labels.txt\",tasks[1]+\"_labels_stem.txt\",\"cr_en_\"+str(j))\n",
    "    rank(datadir[0]+k_dir,tasks[0]+\"_texts.txt\",tasks[0]+\"_combine_labels.txt\",\"cr_en_\"+str(j),tasks[0]+\"_ranked_labels.txt\")\n",
    "    res = p_at_k(datadir[0]+k_dir,tasks[0]+\"_labels_stem.txt\",tasks[0]+\"_ranked_labels.txt\",datadir[0]+k_dir+\"res.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ddd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10e709fa339e96f7843340ac24d9af89d0541623630336e668c3e9cf1c0623cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
