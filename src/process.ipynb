{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/admin02/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2022-10-10 16:57:30,054 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/admin02/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from premethod import *\n",
    "from pegasus_fine_tune import *\n",
    "from generate_pegasus import *\n",
    "from combine import *\n",
    "from rank import rank\n",
    "from rank_training import rank_train\n",
    "from p_at_1 import p_at_k\n",
    "from keybart_finetune import *\n",
    "from keybart_generate import get_pred_Keybart\n",
    "from bart_finetune import fine_tune_bart\n",
    "from bart_generate import get_pred_bart\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = ['../dataset/EUR-Lex/','../dataset/Wiki500K/']\n",
    "k_fold = [0,1,2,3,4]\n",
    "tasks = ['test','train','valid']\n",
    "models = {'pega':1,'bart':0,'kb':0}\n",
    "gener = \"generate_result/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dir:../dataset/Wiki500K/train_finetune.json\n",
      "valid_dir:../dataset/Wiki500K/test_finetune.json\n",
      "model_svae: ../dataset/Wiki500K/pegasus_save\n",
      "model_check: ../dataset/Wiki500K/pegasus_check\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-10 16:57:34,028 - datasets.builder - WARNING - Using custom data configuration default-0be3e47723b3df5d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/admin02/.cache/huggingface/datasets/json/default-0be3e47723b3df5d/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# pegasus\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mif\u001b[39;00m models[\u001b[39m'\u001b[39m\u001b[39mpega\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m----> 3\u001b[0m         Pegasus_fine_tune(datadir[\u001b[39m1\u001b[39;49m],tasks[\u001b[39m1\u001b[39;49m]\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m_finetune.json\u001b[39;49m\u001b[39m'\u001b[39;49m,tasks[\u001b[39m0\u001b[39;49m]\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m_finetune.json\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mpegasus_save\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mpegasus_check\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m):\n\u001b[1;32m      5\u001b[0m             get_pred_Pegasus_fast(datadir[\u001b[39m1\u001b[39m],gener\u001b[39m+\u001b[39mtasks[i]\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_pred.txt\u001b[39m\u001b[39m\"\u001b[39m,tasks[i]\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_finetune.json\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mpegasus_save\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/980pro/XMC/src/pegasus_fine_tune.py:137\u001b[0m, in \u001b[0;36mPegasus_fine_tune\u001b[0;34m(dir, train_dir, valid_dir, model_save, model_check)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[1;32m    136\u001b[0m \u001b[39m# dataset = load_dataset(\"xsum\")\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39;49m\u001b[39mjson\u001b[39;49m\u001b[39m'\u001b[39;49m,data_files\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m: train_dir, \u001b[39m'\u001b[39;49m\u001b[39mvalid\u001b[39;49m\u001b[39m'\u001b[39;49m: valid_dir})\u001b[39m.\u001b[39mshuffle(seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m    138\u001b[0m train_texts, train_labels \u001b[39m=\u001b[39m [prefix \u001b[39m+\u001b[39m each \u001b[39mfor\u001b[39;00m each \u001b[39min\u001b[39;00m dataset[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m'\u001b[39m]], dataset[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39msummary\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    139\u001b[0m valid_texts, valid_labels \u001b[39m=\u001b[39m [prefix \u001b[39m+\u001b[39m each \u001b[39mfor\u001b[39;00m each \u001b[39min\u001b[39;00m dataset[\u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m'\u001b[39m]], dataset[\u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39msummary\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/980pro/anaconda3/envs/dxt/lib/python3.8/site-packages/datasets/load.py:1698\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[1;32m   1695\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   1697\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 1698\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[1;32m   1699\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1700\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1701\u001b[0m     ignore_verifications\u001b[39m=\u001b[39;49mignore_verifications,\n\u001b[1;32m   1702\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[1;32m   1703\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1704\u001b[0m )\n\u001b[1;32m   1706\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1707\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[1;32m   1708\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[1;32m   1709\u001b[0m )\n",
      "File \u001b[0;32m~/980pro/anaconda3/envs/dxt/lib/python3.8/site-packages/datasets/builder.py:807\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m downloaded_from_gcs:\n\u001b[1;32m    802\u001b[0m     prepare_split_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    803\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfile_format\u001b[39m\u001b[39m\"\u001b[39m: file_format,\n\u001b[1;32m    804\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmax_shard_size\u001b[39m\u001b[39m\"\u001b[39m: max_shard_size,\n\u001b[1;32m    805\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[1;32m    806\u001b[0m     }\n\u001b[0;32m--> 807\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m    808\u001b[0m         dl_manager\u001b[39m=\u001b[39;49mdl_manager,\n\u001b[1;32m    809\u001b[0m         verify_infos\u001b[39m=\u001b[39;49mverify_infos,\n\u001b[1;32m    810\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_split_kwargs,\n\u001b[1;32m    811\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdownload_and_prepare_kwargs,\n\u001b[1;32m    812\u001b[0m     )\n\u001b[1;32m    813\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/980pro/anaconda3/envs/dxt/lib/python3.8/site-packages/datasets/builder.py:876\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verify_infos, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    874\u001b[0m split_dict \u001b[39m=\u001b[39m SplitDict(dataset_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m    875\u001b[0m split_generators_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[0;32m--> 876\u001b[0m split_generators \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_split_generators(dl_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msplit_generators_kwargs)\n\u001b[1;32m    878\u001b[0m \u001b[39m# Checksums verification\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[39mif\u001b[39;00m verify_infos \u001b[39mand\u001b[39;00m dl_manager\u001b[39m.\u001b[39mrecord_checksums:\n",
      "File \u001b[0;32m~/980pro/anaconda3/envs/dxt/lib/python3.8/site-packages/datasets/packaged_modules/json/json.py:68\u001b[0m, in \u001b[0;36mJson._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdata_files:\n\u001b[1;32m     67\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAt least one data file must be specified, but got data_files=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdata_files\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m data_files \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39;49mdownload_and_extract(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mdata_files)\n\u001b[1;32m     69\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_files, (\u001b[39mstr\u001b[39m, \u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m     70\u001b[0m     files \u001b[39m=\u001b[39m data_files\n",
      "File \u001b[0;32m~/980pro/anaconda3/envs/dxt/lib/python3.8/site-packages/datasets/download/download_manager.py:433\u001b[0m, in \u001b[0;36mDownloadManager.download_and_extract\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownload_and_extract\u001b[39m(\u001b[39mself\u001b[39m, url_or_urls):\n\u001b[1;32m    418\u001b[0m     \u001b[39m\"\"\"Download and extract given url_or_urls.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \n\u001b[1;32m    420\u001b[0m \u001b[39m    Is roughly equivalent to:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[39m        extracted_path(s): `str`, extracted paths of given URL(s).\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 433\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload(url_or_urls))\n",
      "File \u001b[0;32m~/980pro/anaconda3/envs/dxt/lib/python3.8/site-packages/datasets/download/download_manager.py:310\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    307\u001b[0m download_func \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download, download_config\u001b[39m=\u001b[39mdownload_config)\n\u001b[1;32m    309\u001b[0m start_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[0;32m--> 310\u001b[0m downloaded_path_or_paths \u001b[39m=\u001b[39m map_nested(\n\u001b[1;32m    311\u001b[0m     download_func,\n\u001b[1;32m    312\u001b[0m     url_or_urls,\n\u001b[1;32m    313\u001b[0m     map_tuple\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    314\u001b[0m     num_proc\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mnum_proc,\n\u001b[1;32m    315\u001b[0m     parallel_min_length\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[1;32m    316\u001b[0m     disable_tqdm\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m is_progress_bar_enabled(),\n\u001b[1;32m    317\u001b[0m     desc\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mDownloading data files\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    318\u001b[0m )\n\u001b[1;32m    319\u001b[0m duration \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m    320\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading took \u001b[39m\u001b[39m{\u001b[39;00mduration\u001b[39m.\u001b[39mtotal_seconds() \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m60\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m min\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/980pro/anaconda3/envs/dxt/lib/python3.8/site-packages/datasets/utils/py_utils.py:430\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    426\u001b[0m     num_proc \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    427\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(iterable) \u001b[39m<\u001b[39m parallel_min_length:\n\u001b[1;32m    428\u001b[0m     mapped \u001b[39m=\u001b[39m [\n\u001b[1;32m    429\u001b[0m         _single_map_nested((function, obj, types, \u001b[39mNone\u001b[39;00m, \u001b[39mTrue\u001b[39;00m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m--> 430\u001b[0m         \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m logging\u001b[39m.\u001b[39;49mtqdm(iterable, disable\u001b[39m=\u001b[39;49mdisable_tqdm, desc\u001b[39m=\u001b[39;49mdesc)\n\u001b[1;32m    431\u001b[0m     ]\n\u001b[1;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     num_proc \u001b[39m=\u001b[39m num_proc \u001b[39mif\u001b[39;00m num_proc \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(iterable) \u001b[39melse\u001b[39;00m \u001b[39mlen\u001b[39m(iterable)\n",
      "File \u001b[0;32m~/980pro/anaconda3/envs/dxt/lib/python3.8/site-packages/datasets/utils/logging.py:204\u001b[0m, in \u001b[0;36m_tqdm_cls.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    203\u001b[0m     \u001b[39mif\u001b[39;00m _tqdm_active:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39mreturn\u001b[39;00m tqdm_lib\u001b[39m.\u001b[39;49mtqdm(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    205\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m         \u001b[39mreturn\u001b[39;00m EmptyTqdm(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/notebook.py:239\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m unit_scale \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mor\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m    238\u001b[0m total \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39m*\u001b[39m unit_scale \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal\n\u001b[0;32m--> 239\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstatus_printer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp, total, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdesc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mncols)\n\u001b[1;32m    240\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer\u001b[39m.\u001b[39mpbar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m    241\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplayed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tqdm/notebook.py:112\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m# if not total:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m \n\u001b[1;32m    110\u001b[0m \u001b[39m# Prepare IPython progress bar\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m IProgress \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# #187 #451 #558 #872\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m    113\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIProgress not found. Please update jupyter and ipywidgets.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m See https://ipywidgets.readthedocs.io/en/stable\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m/user_install.html\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[39mif\u001b[39;00m total:\n\u001b[1;32m    117\u001b[0m     pbar \u001b[39m=\u001b[39m IProgress(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39mtotal)\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "# pegasus\n",
    "if models['pega']:\n",
    "        Pegasus_fine_tune(datadir[0],tasks[1]+'_finetune.json',tasks[0]+'_finetune.json',\"pegasus_save\",\"pegasus_check\")\n",
    "        for i in range(2):\n",
    "            get_pred_Pegasus_fast(datadir[1],gener+tasks[i]+\"_pred.txt\",tasks[i]+\"_finetune.json\",\"pegasus_save\")\n",
    "            get_combine_list(datadir[1],gener+tasks[i]+\"_pred.txt\",\"all_stemlabels.txt\",tasks[i]+\"_combine_labels.txt\")\n",
    "        rank_train(datadir[1],tasks[1]+\"_texts.txt\",tasks[1]+\"_combine_labels.txt\",tasks[1]+\"_labels_stem.txt\",\"cr_en\")\n",
    "        rank(datadir[1],tasks[0]+\"_texts.txt\",tasks[0]+\"_combine_labels.txt\",\"cr_en\",tasks[0]+\"_ranked_labels.txt\")\n",
    "        res = p_at_k(datadir[1],tasks[0]+\"_labels_stem.txt\",tasks[0]+\"_ranked_labels.txt\",datadir[1]+\"res_pega.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if models['bart']:\n",
    "        fine_tune_bart(datadir[0],tasks[1]+'_finetune.json',tasks[0]+'_finetune.json','bart_save','bart_check')\n",
    "        for i in range(2):\n",
    "            get_pred_bart(datadir[0],gener+tasks[i]+\"_pred_ba.txt\",tasks[i]+\"_finetune.json\",\"bart_save\")\n",
    "            bart_clean(datadir[0]+gener+tasks[i]+\"_pred_ba.txt\",datadir[0]+gener+tasks[i]+\"_pred_ba_c.txt\")\n",
    "            get_combine_list(datadir[0],gener+tasks[i]+\"_pred_ba_c.txt\",\"all_stemlabels.txt\",gener+tasks[i]+\"_combine_labels_ba.txt\")\n",
    "        rank_train(datadir[0],tasks[1]+\"_texts.txt\",gener+tasks[1]+\"_combine_labels_ba.txt\",tasks[1]+\"_labels_stem.txt\",\"cr_en_ba\")\n",
    "        rank(datadir[0],tasks[0]+\"_texts.txt\",gener+tasks[0]+\"_combine_labels_ba.txt\",\"cr_en_ba\",gener+tasks[0]+\"_ranked_labels_ba.txt\")\n",
    "        res = p_at_k(datadir[0],tasks[0]+\"_labels_stem.txt\",gener+tasks[0]+\"_ranked_labels_ba.txt\",datadir[0]+\"ba_res.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if models['kb']:    \n",
    "        kb_fine_tune(datadir[0],\"kb_save\",\"kb_check\")\n",
    "        fine_tune_keybart(datadir[0],tasks[1]+'_finetune.json',tasks[0]+'_finetune.json','keybart_save','keybart_test')\n",
    "        for i in range(2):\n",
    "            get_pred_Keybart(datadir[0],gener+tasks[i]+\"_pred_kb.txt\",tasks[i]+\"_finetune.json\",\"keybart_save\")\n",
    "            bart_clean(datadir[0]+gener+tasks[i]+\"_pred_kb.txt\",datadir[0]+gener+tasks[i]+\"_pred_kb_c.txt\")\n",
    "            get_combine_list(datadir[0],gener+tasks[i]+\"_pred_kb_c.txt\",\"all_labels_sterm.txt\",gener+tasks[i]+\"_combine_labels_kb.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K则验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in k_fold:\n",
    "    k_dir = \"/K_fold/\"+\"K_\"+str(j)+\"/\"\n",
    "    Pegasus_fine_tune(datadir[0]+k_dir,tasks[1]+'.json',tasks[0]+'.json',\"pegasus_save\",\"pegasus_check\")\n",
    "    for i in range(2):\n",
    "        get_pred_Pegasus(datadir[0]+k_dir,tasks[i]+\"_pred.txt\",tasks[i]+\".json\",\"pegasus_save\")\n",
    "        get_combine_list(datadir[0]+k_dir,tasks[i]+\"_pred.txt\",\"all_stemlabels.txt\",tasks[i]+\"_combine_labels.txt\")\n",
    "    rank_train(datadir[0]+k_dir,tasks[1]+\"_texts.txt\",tasks[1]+\"_combine_labels.txt\",tasks[1]+\"_labels_stem.txt\",\"cr_en_\"+str(j))\n",
    "    rank(datadir[0]+k_dir,tasks[0]+\"_texts.txt\",tasks[0]+\"_combine_labels.txt\",\"cr_en_\"+str(j),tasks[0]+\"_ranked_labels.txt\")\n",
    "    res = p_at_k(datadir[0]+k_dir,tasks[0]+\"_labels_stem.txt\",tasks[0]+\"_ranked_labels.txt\",datadir[0]+k_dir+\"res.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dxt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ac9529b4fab9e834390346e7466129441a36f5aa680424ec2e52c810b15a064"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
